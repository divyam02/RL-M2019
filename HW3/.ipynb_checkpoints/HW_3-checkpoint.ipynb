{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.\n",
    "\n",
    "```\n",
    "# Psuedo Code from Monte Carlo with Exploring Starts\n",
    "\n",
    "Init:\n",
    "    pi(s);         chosen from A(s) for all states s. \n",
    "    Q(s,a);        chosen from R for all state-action pairs (s,a).\n",
    "    Returns(s,a);  dictionary of mean returns (single value).\n",
    "    Counts(s,a);   List to keep track of state-actions encountered.\n",
    "    \n",
    "While True(for each episode):\n",
    "    Choose (s,a) pair so that all such pairs are picked with a non-zero probability every episode. \n",
    "    Generate episode from (S0,A0) under pi.\n",
    "    Init G=0\n",
    "    for t in range(0, T, -1):\n",
    "        G = discount*G + R(t+1)\n",
    "        Unless (St,At) appear in remaining iterations:\n",
    "            Returns(s,a) = Returns(s,a) + [G - Returns(s,a)]/Counts(s,a)\n",
    "            Q(St,At) = Returns(St,At)\n",
    "            pi(St) = argmax(a, Q(St,At))\n",
    "            Counts(St,At) += 1\n",
    "```\n",
    "\n",
    "* This incremental update is equivalent to averaging over all returns as we are weighting the total returns ,`Returns(St,At)` by `[(n-1)/n]` and the current return `G` by `1/n`. the where `n` is the number of episodes or times it has been visited (depending on First-Visit of Every-Visit MC).\n",
    "* This is exactly what we do during averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Monte Carlo base class.\n",
    "\"\"\"\n",
    "class MonteCarlo():\n",
    "    def __init__(self, states, actions, policy, gamma):\n",
    "        self.state_values = np.zeros(shape=(len(states)))\n",
    "        self.returns = [[] for i in range(len(states))]\n",
    "#         Hit is +1 and stick is -1\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def run_episode(self):\n",
    "        states, actions, rewards = gen_episode()\n",
    "        G = 0\n",
    "        for i in range(0, len(states), -1):\n",
    "            G = self.gamma*G + rewards[i]\n",
    "            if not states.index(states[i])==i:\n",
    "                self.returns[states[i]].append(G)\n",
    "                self.state_values[states[i]] = sum(self.returns[states[i]])/len(self.returns[states[i]])\n",
    "                \n",
    "    def gen_episode(self, model, horizon):\n",
    "        states = list()\n",
    "        actions = list()\n",
    "        rewards = list()\n",
    "        \n",
    "#         state[0] = ?\n",
    "        action[0] = self.policy(state[0])\n",
    "        \n",
    "        for i in range(horizon):\n",
    "            a, c = model.play(states[-1], actions[-1]) # Returns new state and reward.\n",
    "            states.append(a)\n",
    "#             actions.append(b)\n",
    "            rewards.append(c)\n",
    "            actions.append(self.policy(self.states[-1])) # Choose new action as per policy\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blackjack code used for Q4.\n",
    "\"\"\"\n",
    "class BlackjackGame():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Let hit be a 1\n",
    "        and stick be -1\n",
    "        \"\"\"\n",
    "        self.policy = np.zeros(22)\n",
    "        for i in range(0, 17):\n",
    "            self.policy[i] = 1\n",
    "        for i in range(17, 22):\n",
    "            self.policy[i] = -1\n",
    "            \n",
    "        self.player_total = 0\n",
    "        self.dealer_total = 0\n",
    "        self.dealer_cards = (0, 0)\n",
    "        self.player_usable_ace = False\n",
    "    \n",
    "    def deal_card(self):\n",
    "#         There are 13 unique card faces in total\n",
    "    card = np.random.randint(1, 14)\n",
    "    if card==1:\n",
    "        return 11\n",
    "    else:\n",
    "        return 10\n",
    "    \n",
    "    def play(self, state, action):\n",
    "#         State format: (current total, dealer card viewable, has a usable ace)\n",
    "        if action=='hit':\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
