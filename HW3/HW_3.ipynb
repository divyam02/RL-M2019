{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.\n",
    "\n",
    "```\n",
    "# Psuedo Code from Monte Carlo with Exploring Starts\n",
    "\n",
    "Init:\n",
    "    pi(s);         chosen from A(s) for all states s. \n",
    "    Q(s,a);        chosen from R for all state-action pairs (s,a).\n",
    "    Returns(s,a);  dictionary of mean returns (single value).\n",
    "    Counts(s,a);   List to keep track of state-actions encountered.\n",
    "    \n",
    "While True(for each episode):\n",
    "    Choose (s,a) pair so that all such pairs are picked with a non-zero probability every episode. \n",
    "    Generate episode from (S0,A0) under pi.\n",
    "    Init G=0\n",
    "    for t in range(0, T, -1):\n",
    "        G = discount*G + R(t+1)\n",
    "        Unless (St,At) appear in remaining iterations:\n",
    "            Returns(s,a) = Returns(s,a) + [G - Returns(s,a)]/Counts(s,a)\n",
    "            Q(St,At) = Returns(St,At)\n",
    "            pi(St) = argmax(a, Q(St,At))\n",
    "            Counts(St,At) += 1\n",
    "```\n",
    "\n",
    "* This incremental update is equivalent to averaging over all returns as we are weighting the total returns ,`Returns(St,At)` by `[(n-1)/n]` and the current return `G` by `1/n`. the where `n` is the number of episodes or times it has been visited (depending on First-Visit of Every-Visit MC).\n",
    "* This is exactly what we do during averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Monte Carlo base class.\n",
    "\"\"\"\n",
    "class MonteCarlo():\n",
    "    def __init__(self, states, actions, policy, gamma):\n",
    "        self.state_values = np.zeros(shape=(len(states)))\n",
    "        self.returns = [[] for i in range(len(states))]\n",
    "#         Hit is +1 and stick is -1\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def run_episode(self):\n",
    "        states, actions, rewards = gen_episode()\n",
    "        G = 0\n",
    "        for i in range(0, len(states), -1):\n",
    "            G = self.gamma*G + rewards[i]\n",
    "            if not states.index(states[i])==i:\n",
    "                self.returns[states[i]].append(G)\n",
    "                self.state_values[states[i]] = sum(self.returns[states[i]])/len(self.returns[states[i]])\n",
    "                \n",
    "    def gen_episode(self, model, horizon):\n",
    "        states = list()\n",
    "        actions = list()\n",
    "        rewards = list()\n",
    "        \n",
    "#         state[0] = ?\n",
    "        action[0] = self.policy(state[0])\n",
    "        \n",
    "        for i in range(horizon):\n",
    "            a, c = model.play(states[-1], actions[-1]) # Returns new state and reward.\n",
    "            states.append(a)\n",
    "#             actions.append(b)\n",
    "            rewards.append(c)\n",
    "            actions.append(self.policy(self.states[-1])) # Choose new action as per policy\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blackjack code used for Q4.\n",
    "\"\"\"\n",
    "class BlackjackGame():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Let hit be a 1\n",
    "        and stick be -1\n",
    "        \"\"\"\n",
    "        self.policy = np.zeros(22)\n",
    "        for i in range(0, 17):\n",
    "            self.policy[i] = 1\n",
    "        for i in range(17, 22):\n",
    "            self.policy[i] = -1\n",
    "            \n",
    "        self.player_total = 0\n",
    "        self.dealer_total = 0\n",
    "        self.dealer_cards = (0, 0)\n",
    "        self.player_usable_ace = False\n",
    "        self.usable_ace_counter = 0\n",
    "    \n",
    "    def deal_card(self):\n",
    "#         There are 13 unique card faces in total\n",
    "    card = np.random.randint(1, 14)\n",
    "    if card==1:\n",
    "        return 11\n",
    "    else:\n",
    "        return min(card, 10)\n",
    "    \n",
    "    def play(self, initial_state, action):\n",
    "#         State format: (current total, dealer card viewable, has a usable ace)\n",
    "        self.player_total, self.deal_card_viewable, self.player_usable_ace = initial_state\n",
    "        self.deal_card[0] = self.deal_card_viewable\n",
    "        self.dealer_card[1] = self.deal_card()\n",
    "        \n",
    "        self.dealer_total = self.dealer_card[0] + self.deal_card[1]\n",
    "        if self.dealer_total == 22:\n",
    "            self.dealer_total -= 10 # Treat 11 as 1.\n",
    "        \n",
    "#         Player turn\n",
    "        if self.player_total==0:\n",
    "            self.player_total = self.deal_card() + self.deal_card()\n",
    "        if self.player_total==22:\n",
    "            self.player_total -= 10 # Treat 11 as 1\n",
    "            \n",
    "        if action == 1:\n",
    "            new_card = self.deal_card()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SARSA and Q-Learning\n",
    "\"\"\"\n",
    "def Q_learning(terminals, states_size, actions_size, policy, model, start\n",
    "               episodes=500, alpha=0.1, gamma=1):\n",
    "    \"\"\"\n",
    "    Assume states are numbered 0 to whatever\n",
    "    Directions are 0, 1, 2 ,3 for up, left, down, right.\n",
    "    \"\"\"\n",
    "    Q_values = np.zeros(shape=(states_size, actions_size))\n",
    "    \n",
    "    accumulated_rewards = np.zeros(shape=(episodes, 1))\n",
    "    \n",
    "    for episode in episodes:\n",
    "        s = start\n",
    "        \n",
    "        while(not s in terminals):\n",
    "            a = policy(Q_values, s)\n",
    "            s_next, r = model(s, a)\n",
    "            a_greedy = np.argmax(Q_values[s_next, :])\n",
    "            Q_values[s, a] += alpha*(r + gamma*Q_values[s_next, a_greedy] - Q_values[s, a])\n",
    "            s = s_next\n",
    "        \n",
    "        try:\n",
    "            accumulated_rewards[episode] = np.sum(Q_values)\n",
    "            accumulated_rewards[episode] -= accumulated_rewards[episode-1]\n",
    "        except:\n",
    "            accumulated_rewards[episode] = np.sum(Q_values)\n",
    "            \n",
    "        \n",
    "def SARSA(terminals, states_size, actions_size, policy, model, start\n",
    "               episodes=500, alpha=0.1, gamma=1):\n",
    "    \"\"\"\n",
    "    Assume states are numbered 0 to whatever\n",
    "    Directions are 0, 1, 2 ,3 for up, left, down, right.\n",
    "    \"\"\"\n",
    "    Q_values = np.zeros(shape=(states_size, actions_size))\n",
    "    \n",
    "    accumulated_rewards = np.zeros(shape=(episodes, 1))\n",
    "    \n",
    "    for episode in episodes:\n",
    "        s = start\n",
    "        a = policy(Q_values, s)\n",
    "        \n",
    "        while(not s in terminals):\n",
    "            s_next, r = model(s, a)\n",
    "            a_greedy = policy(Q_values, s_next)\n",
    "            Q_values[s, a] += alpha*(r + gamma*Q_values[s_next, a_greedy] - Q_values[s, a])\n",
    "            s = s_next\n",
    "            a = a_greedy\n",
    "        \n",
    "        try:\n",
    "            accumulated_rewards[episode] = np.sum(Q_values)\n",
    "            accumulated_rewards[episode] -= accumulated_rewards[episode-1]\n",
    "        except:\n",
    "            accumulated_rewards[episode] = np.sum(Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code used in Q7.\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "class cliffWalking():\n",
    "    def __init__(self):\n",
    "        states_size = 48\n",
    "        action_size = 4\n",
    "        terminals = [37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
    "        \n",
    "    def model(state, action):\n",
    "        if action==0:\n",
    "            state -= 12\n",
    "            if state < 0:\n",
    "                state += 12\n",
    "                \n",
    "        elif action==2:\n",
    "            state += 12\n",
    "            if state >= 48:\n",
    "                state -= 12\n",
    "        \n",
    "        elif action==1:\n",
    "            if not(state==36 or state==24 or state==12 or state==0):\n",
    "                state -= 1\n",
    "            \n",
    "        elif action==3:\n",
    "            if not(state==11 or state==23 or state==35 or state==47):\n",
    "                state+=1\n",
    "        \n",
    "        else:\n",
    "            print(\"weird action:\", action)\n",
    "            \n",
    "        if state>36 and state<47:\n",
    "            reward = -100\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        return state, reward\n",
    "            \n",
    "    def policy(Q_vals, state, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        The epsilon greedy policy.\n",
    "        Q_vals: 2D numpy array\n",
    "        state: int 0 to 47\n",
    "        \"\"\"\n",
    "\n",
    "        p = random.random()\n",
    "\n",
    "        if p <= epsilon:\n",
    "            return random.randint(0, 3)\n",
    "\n",
    "        else:\n",
    "            return np.argmax(Q_vals[state])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
