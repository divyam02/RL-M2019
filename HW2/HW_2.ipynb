{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used for Q4. Represents the grid world of fig. 3.2 and 3.5.\n",
    "\"\"\"\n",
    "class gridWorld_1():\n",
    "    def __init__(self, grid_size=5, actions=['N', 'S', 'E', 'W'], gamma=0.9):\n",
    "        self.grid_size = grid_size\n",
    "        self.actions = actions\n",
    "        self.grid_values = np.zeros((grid_size, grid_size))\n",
    "        self.prev_values = np.zeros((grid_size, grid_size))\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_new_state_reward(self, state, action):\n",
    "        \"\"\"\n",
    "        input state tuple and action char\n",
    "        \n",
    "        Returns state, reward\n",
    "        \"\"\"\n",
    "        x, y = state\n",
    "        if (x==0 and y==0) and (action=='N' or action=='W'):\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif (x==0 and y==self.grid_size-1) and (action=='N' or action=='E'):\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif (x==self.grid_size-1 and y==self.grid_size-1) and (action=='S' or action=='E'):\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif (x==self.grid_size-1 and y==0) and (action=='S' or action=='W'):\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif x==0 and action=='N':\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif x==self.grid_size-1 and action=='S':\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif y==0 and action=='W':\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif y==self.grid_size-1 and action=='E':\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif x==0 and y==1:\n",
    "            return (4, 1), 10\n",
    "        \n",
    "        elif x==0 and y==3:\n",
    "            return (2, 3), 5\n",
    "        \n",
    "        elif action=='N':\n",
    "            return (x-1, y), 0\n",
    "        elif action=='S':\n",
    "            return (x+1, y), 0\n",
    "        elif action=='E':\n",
    "            return (x, y+1), 0\n",
    "        elif action=='W':\n",
    "            return (x, y-1), 0\n",
    "        \n",
    "        else:\n",
    "            print(\"Unknown state or action:\", state, action)\n",
    "            \n",
    "    def get_new_value_estimate(self, i, j):\n",
    "#         temp = 0\n",
    "        prev_max = -np.inf\n",
    "        for action in self.actions:\n",
    "            a, b = self.get_new_state_reward((i, j), action)\n",
    "            c = self.gamma * self.grid_values[a[0], a[1]] + b\n",
    "            if prev_max < c:\n",
    "                prev_max = c\n",
    "            \n",
    "        return prev_max\n",
    "            \n",
    "    def value_iteration(self):\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                self.grid_values[i, j] = self.get_new_value_estimate(i, j)\n",
    "                \n",
    "    def get_grid(self):\n",
    "        for i in range(10000):\n",
    "            self.value_iteration()\n",
    "#             print(\"Iteration:\", i)\n",
    "        print(np.round(self.grid_values, decimals=1), \"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.3  8.8  4.4  5.3  1.5]\n",
      " [ 1.5  3.   2.3  1.9  0.5]\n",
      " [ 0.1  0.7  0.7  0.4 -0.4]\n",
      " [-1.  -0.4 -0.4 -0.6 -1.2]\n",
      " [-1.9 -1.3 -1.2 -1.4 -2. ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q2. \n",
    "\n",
    "We attempt to solve the system of 25 linear equations (1 equation per state)\n",
    "using Ax = b, where A is a matrix of coefficients of the equations, x is a\n",
    "a vector of all state values and b is a vector of constants obtained from the \n",
    "system of equations.\n",
    "\n",
    "This can be done using the numpy library.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "\n",
    "A = np.zeros((25, 25), dtype=np.float32)\n",
    "b = np.zeros((25,), dtype=np.float32)\n",
    "with open('variable_coeffs.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "for i, line in enumerate(lines):\n",
    "    A[i] = np.asarray(list(map(float, line.split(\",\"))))\n",
    "    \n",
    "with open('constants.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "for i, line in enumerate(lines):\n",
    "    b[i] = np.asarray(list(map(float, line.split(\",\"))))\n",
    "    \n",
    "# print(A)\n",
    "# print(b)\n",
    "\n",
    "x = linalg.solve(A, b)\n",
    "x = np.reshape(x, (5, 5))\n",
    "print(np.round(x, decimals=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.  24.4 22.  19.4 17.5]\n",
      " [19.8 22.  19.8 17.8 16. ]\n",
      " [17.8 19.8 17.8 16.  14.4]\n",
      " [16.  17.8 16.  14.4 13. ]\n",
      " [14.4 16.  14.4 13.  11.7]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q4. \n",
    "\n",
    "We solve for optimal state values by picking actions corresponding max return of rewards and solving \n",
    "Belman equations using Value Iteration. I iterate for 10,000 steps considering gamma^10,000 ~ 0 hence \n",
    "rewards will have converged as well. \n",
    "\"\"\"\n",
    "A = gridWorld_1()\n",
    "A.get_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"\n",
    " Used in Q6. Represents the grid world of example 4.1\n",
    " \"\"\"\n",
    "import random\n",
    "\n",
    "class gridWorld_2():\n",
    "    def __init__(self, grid_size=4, actions=['N', 'S', 'E', 'W'], gamma=1):\n",
    "        self.grid_size = grid_size\n",
    "        self.actions = actions\n",
    "        self.grid_values = np.zeros((grid_size, grid_size))\n",
    "        self.prev_values = np.zeros((grid_size, grid_size))\n",
    "        self.gamma = gamma\n",
    "        self.init_policy = {(0, 0):'S', (0, 1):'S', (0, 2):'S', (0, 3):'S', (0, 4):'S',\n",
    "                            (1, 0):'S', (1, 1):'S', (1, 2):'S', (1, 3):'S', (1, 4):'S',\n",
    "                            (2, 0):'S', (2, 1):'S', (2, 2):'S', (2, 3):'S', (3, 4):'S',\n",
    "                            (3, 0):'S', (3, 1):'S', (3, 2):'S', (3, 3):'S', (4, 4):'S',}\n",
    "        \n",
    "    def get_new_state_reward(self, state, action):\n",
    "        \"\"\"\n",
    "        input state tuple and action char\n",
    "        \n",
    "        Returns state, reward\n",
    "        \"\"\"\n",
    "        x, y = state\n",
    "        if (x==0 and y==0):\n",
    "            # Terminal State!\n",
    "            return (x, y), 0\n",
    "        \n",
    "        elif (x==0 and y==self.grid_size-1) and (action=='N' or action=='E'):\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif (x==self.grid_size-1 and y==self.grid_size-1):\n",
    "            # Terminal State!\n",
    "            return (x, y), 0\n",
    "        \n",
    "        elif (x==self.grid_size-1 and y==0) and (action=='S' or action=='W'):\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif x==0 and action=='N':\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif x==self.grid_size-1 and action=='S':\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif y==0 and action=='W':\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif y==self.grid_size-1 and action=='E':\n",
    "            return (x, y), -1\n",
    "        \n",
    "        elif action=='N':\n",
    "            return (x-1, y), -1\n",
    "        elif action=='S':\n",
    "            return (x+1, y), -1\n",
    "        elif action=='E':\n",
    "            return (x, y+1), -1\n",
    "        elif action=='W':\n",
    "            return (x, y-1), -1\n",
    "        \n",
    "        else:\n",
    "            print(\"Unknown state or action:\", state, action)    \n",
    "        \n",
    "    def get_optimal_value_estimate(self, i, j):\n",
    "        \"\"\"\n",
    "        We take advantage of the fact that my MDP, P(s', r | s, a) for various s and a\n",
    "        in the current question is deterministic, ie either 0 or 1. It is convinient to \n",
    "        consider only the states that are reachable by action ('up', 'down', 'left', 'right')\n",
    "        in the estimation of a given state and simply ignore all other states (as they are \n",
    "        not reachable, P(s', r | s, a) for those states is actually 0).\n",
    "        \"\"\"\n",
    "        prev_max = -np.inf\n",
    "        for action in self.actions:\n",
    "            a, b = self.get_new_state_reward((i, j), action)\n",
    "            c = self.gamma * self.grid_values[a[0], a[1]] + b\n",
    "            if prev_max < c:\n",
    "                prev_max = c\n",
    "            \n",
    "        return prev_max\n",
    "            \n",
    "    def policy_improvement(self, policy, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        @policy:\n",
    "            A dict() object containing state keys and optimal action values.\n",
    "        @state = (i, j)\n",
    "        \n",
    "        Internally updates self.grid_values to optimal and returns \n",
    "        optimal_state_values (copy of self.grid_values) and optimal_policy\n",
    "        \n",
    "        We take advantage of the fact that the MDP here is deterministic,\n",
    "        meaning given a state and a chosen action, P(s', r | s, a) = 1 or 0. This\n",
    "        is because it mentioned in the question that the agent will go in the direction\n",
    "        as per the action with probability 1. Hence for the action chosen by the policy \n",
    "        we recieve a reward from the `get_new_state_reward()` method with probability 1.\n",
    "        \n",
    "        We fix contest between equally good policies by ignoring a policy giving equal \n",
    "        rewards as another.\n",
    "        \"\"\"\n",
    "        stable_policy = True\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                if (i, j)!=(0, 0) and (i, j)!=(self.grid_size-1, self.grid_size-1):\n",
    "                    \n",
    "                    old_action = policy[(i, j)]\n",
    "                    new_action = None\n",
    "                    prev_max = -np.inf\n",
    "                    for action in self.actions:\n",
    "\n",
    "                        a, b = self.get_new_state_reward((i, j), action)\n",
    "                        c = self.gamma * self.grid_values[a[0], a[1]] + b\n",
    "\n",
    "                        # This should fix contest between two equally good \n",
    "                        # policies for a given state...\n",
    "                        if prev_max < c:\n",
    "                            prev_max = c\n",
    "                            new_action = action\n",
    "                            \n",
    "#                     print(\"prev_max\", prev_max)\n",
    "#                     print(\"old, new\", old_action, new_action)\n",
    "#                     print(self.grid_values, \"\\n\")\n",
    "\n",
    "                    policy[(i, j)] = new_action\n",
    "                    if new_action!=old_action:\n",
    "                        stable_policy = False\n",
    "        \n",
    "        print(\"Improved policy:\")\n",
    "        print(self.grid_values)\n",
    "        print(policy)\n",
    "        \n",
    "        if not stable_policy:\n",
    "            print(\"Policy not stable yet...\", \"\\n\")\n",
    "            self.policy_evaluation(policy)\n",
    "            return self.policy_improvement(policy)\n",
    "            \n",
    "        else:\n",
    "            print(\"Policy stable!\", \"\\n\")\n",
    "            return policy\n",
    "                \n",
    "        \n",
    "    def policy_evaluation(self, policy, epsilon=1e-6, random_policy=False):\n",
    "        \"\"\"\n",
    "        @policy: \n",
    "            A dict() object containing state keys and optimal action values.\n",
    "            \n",
    "        We again take advantage of the fact that the MDP here is deterministic,\n",
    "        meaning given a state and a chosen action, P(s', r | s, a) = 1 or 0. This\n",
    "        is because it mentioned in the question that the agent will go in the direction\n",
    "        as per the action with probability 1. Hence for the action chosen by the policy \n",
    "        we recieve a reward from the `get_new_state_reward()` method with probability 1.\n",
    "        \n",
    "        Problem: Current evaluation is indefinite because iteratively the policy keeps decreasing \n",
    "        the value and it never converges...\n",
    "        \n",
    "        We solve this by keeping the initial policy as equiprobable.\n",
    "        \n",
    "        \"\"\"\n",
    "        flag = [False]*self.grid_size*self.grid_size\n",
    "        flag[0] = True\n",
    "        flag[15] = True\n",
    "        counter = 0\n",
    "        while True:\n",
    "            for i in range(self.grid_size):\n",
    "                for j in range(self.grid_size):\n",
    "                \n",
    "                    init_state_value = np.copy(self.grid_values[i, j])\n",
    "                    if random_policy:\n",
    "                        for action in self.actions:\n",
    "                            a, b = self.get_new_state_reward((i, j), action)\n",
    "                            self.grid_values[i, j] = 0.25 * (b + self.gamma * self.grid_values[a[0], a[1]])\n",
    "                    else:\n",
    "                        a, b = self.get_new_state_reward((i, j), policy[(i, j)])\n",
    "                        self.grid_values[i, j] = b + self.gamma * self.grid_values[a[0], a[1]]\n",
    "\n",
    "                    delta = abs(self.grid_values[i, j] - init_state_value)\n",
    "\n",
    "                    counter +=1\n",
    "#                     if counter%100==0:\n",
    "#                         print(counter)\n",
    "#                         print(self.grid_values)\n",
    "#                         print(policy)\n",
    "#                         print(flag)\n",
    "#                      input()\n",
    "                    \n",
    "                    if delta <= epsilon:\n",
    "                        if flag[i*self.grid_size + j]==False:\n",
    "                            print(\"Converged!\", i*self.grid_size + j + 1)\n",
    "                        flag[i*self.grid_size + j] = True\n",
    "                        \n",
    "                    if all(i==True for i in flag):\n",
    "                        print(\"Reached break point!\")\n",
    "                        return\n",
    "    \n",
    "    def value_iteration(self, epsilon=1e-5):\n",
    "        flag = [False]*self.grid_size*self.grid_size\n",
    "        flag[0] = True\n",
    "        flag[15] = True\n",
    "        counter = 0\n",
    "        while not all(i==True for i in flag):\n",
    "            for i in range(self.grid_size):\n",
    "                for j in range(self.grid_size):\n",
    "                    if flag[i*self.grid_size + j]==False:\n",
    "                        \n",
    "                        init_state_value = np.copy(self.grid_values[i, j])\n",
    "                        self.grid_values[i, j] = self.get_optimal_value_estimate(i, j)\n",
    "                        if abs(init_state_value - self.grid_values[i, j] <= epsilon):\n",
    "                            \n",
    "                            print(\"State\", str(1 + i*4 + j), \"has converged...\")\n",
    "                            flag[i*self.grid_size + j] = True\n",
    "                            \n",
    "            print(\"Iteration:\", counter+1)\n",
    "            print(\"State Values:\")\n",
    "            print(self.grid_values)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            counter+=1\n",
    "                \n",
    "    def solve_by_policy_iteration(self):\n",
    "        init_policy = {(0, 0):None, (0, 1):None, (0, 2):None, (0, 3):None,\n",
    "                       (1, 0):None, (1, 1):None, (1, 2):None, (1, 3):None,\n",
    "                       (2, 0):None, (2, 1):None, (2, 2):None, (2, 3):None,\n",
    "                       (3, 0):None, (3, 1):None, (3, 2):None, (3, 3):None}\n",
    "        \n",
    "\n",
    "        self.policy_evaluation(init_policy, random_policy=True)\n",
    "        policy = self.policy_improvement(init_policy)\n",
    "        return policy\n",
    "        \n",
    "#         print(np.round(self.grid_values, decimals=1), \"\\n\")\n",
    "        \n",
    "    def solve_by_value_iteration(self):\n",
    "        self.value_iteration()\n",
    "        print(\"Optimal State Values\")\n",
    "        print(np.round(self.grid_values, decimals=1), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged! 2\n",
      "Converged! 3\n",
      "Converged! 4\n",
      "Converged! 7\n",
      "Converged! 8\n",
      "Converged! 11\n",
      "Converged! 12\n",
      "Converged! 15\n",
      "Converged! 5\n",
      "Converged! 6\n",
      "Converged! 9\n",
      "Converged! 10\n",
      "Converged! 13\n",
      "Converged! 14\n",
      "Reached break point!\n",
      "Improved policy:\n",
      "[[ 0.         -0.25       -0.3125     -0.328125  ]\n",
      " [-0.33333333 -0.33333333 -0.33333333 -0.33333333]\n",
      " [-0.33333333 -0.33333333 -0.33333333 -0.33333333]\n",
      " [-0.33333333 -0.33333333 -0.33333333  0.        ]]\n",
      "{(0, 0): None, (0, 1): 'W', (0, 2): 'W', (0, 3): 'W', (1, 0): 'N', (1, 1): 'N', (1, 2): 'N', (1, 3): 'N', (2, 0): 'N', (2, 1): 'W', (2, 2): 'S', (2, 3): 'S', (3, 0): 'N', (3, 1): 'E', (3, 2): 'E', (3, 3): None}\n",
      "Policy not stable yet... \n",
      "\n",
      "Converged! 2\n",
      "Converged! 3\n",
      "Converged! 4\n",
      "Converged! 5\n",
      "Converged! 6\n",
      "Converged! 7\n",
      "Converged! 8\n",
      "Converged! 9\n",
      "Converged! 10\n",
      "Converged! 12\n",
      "Converged! 13\n",
      "Converged! 15\n",
      "Converged! 11\n",
      "Converged! 14\n",
      "Reached break point!\n",
      "Improved policy:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "{(0, 0): None, (0, 1): 'W', (0, 2): 'W', (0, 3): 'W', (1, 0): 'N', (1, 1): 'N', (1, 2): 'N', (1, 3): 'S', (2, 0): 'N', (2, 1): 'N', (2, 2): 'S', (2, 3): 'S', (3, 0): 'N', (3, 1): 'E', (3, 2): 'E', (3, 3): None}\n",
      "Policy not stable yet... \n",
      "\n",
      "Converged! 2\n",
      "Converged! 3\n",
      "Converged! 4\n",
      "Converged! 5\n",
      "Converged! 6\n",
      "Converged! 7\n",
      "Converged! 9\n",
      "Converged! 10\n",
      "Converged! 11\n",
      "Converged! 12\n",
      "Converged! 13\n",
      "Converged! 14\n",
      "Converged! 15\n",
      "Converged! 8\n",
      "Reached break point!\n",
      "Improved policy:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "{(0, 0): None, (0, 1): 'W', (0, 2): 'W', (0, 3): 'S', (1, 0): 'N', (1, 1): 'N', (1, 2): 'N', (1, 3): 'S', (2, 0): 'N', (2, 1): 'N', (2, 2): 'S', (2, 3): 'S', (3, 0): 'N', (3, 1): 'E', (3, 2): 'E', (3, 3): None}\n",
      "Policy not stable yet... \n",
      "\n",
      "Converged! 2\n",
      "Converged! 3\n",
      "Converged! 4\n",
      "Converged! 5\n",
      "Converged! 6\n",
      "Converged! 7\n",
      "Converged! 8\n",
      "Converged! 9\n",
      "Converged! 10\n",
      "Converged! 11\n",
      "Converged! 12\n",
      "Converged! 13\n",
      "Converged! 14\n",
      "Converged! 15\n",
      "Reached break point!\n",
      "Improved policy:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "{(0, 0): None, (0, 1): 'W', (0, 2): 'W', (0, 3): 'S', (1, 0): 'N', (1, 1): 'N', (1, 2): 'N', (1, 3): 'S', (2, 0): 'N', (2, 1): 'N', (2, 2): 'S', (2, 3): 'S', (3, 0): 'N', (3, 1): 'E', (3, 2): 'E', (3, 3): None}\n",
      "Policy stable! \n",
      "\n",
      "Optimal Policy: {(0, 0): None, (0, 1): 'W', (0, 2): 'W', (0, 3): 'S', (1, 0): 'N', (1, 1): 'N', (1, 2): 'N', (1, 3): 'S', (2, 0): 'N', (2, 1): 'N', (2, 2): 'S', (2, 3): 'S', (3, 0): 'N', (3, 1): 'E', (3, 2): 'E', (3, 3): None}\n",
      "Optimal State values:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "A = gridWorld_2()\n",
    "optimal_policy = A.solve_by_policy_iteration()\n",
    "print(\"Optimal Policy:\", optimal_policy)\n",
    "print(\"Optimal State values:\")\n",
    "print(A.grid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "State Values:\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "State 2 has converged...\n",
      "State 6 has converged...\n",
      "State 14 has converged...\n",
      "State 18 has converged...\n",
      "Iteration: 2\n",
      "State Values:\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "State 3 has converged...\n",
      "State 7 has converged...\n",
      "State 9 has converged...\n",
      "State 11 has converged...\n",
      "State 13 has converged...\n",
      "State 17 has converged...\n",
      "Iteration: 3\n",
      "State Values:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "State 4 has converged...\n",
      "State 8 has converged...\n",
      "State 12 has converged...\n",
      "State 16 has converged...\n",
      "Iteration: 4\n",
      "State Values:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "Optimal State Values\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "B = gridWorld_2()\n",
    "B.solve_by_value_iteration()\n",
    "# print(\"Optimal Policy:\", optimal_policy)\n",
    "# print(\"Optimal State values:\")\n",
    "# print(B.grid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q7.\n",
    "\n",
    "Referenced how to make heatmap online (https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/blob/4d8928bb7538d81b818267c983f3fd004ffc9068/chapter04/car_rental.py#L124)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import poisson as poisson\n",
    "matplotlib.use('Agg')\n",
    "class JacksCarRental():\n",
    "    def __init__(self, init_1=10, init_2=10, rent_lambda_1=3, rent_lambda_2=4, \n",
    "                 return_lambda_1=3, return_lambda_2=2, gamma=0.9):\n",
    "        self.loc_1 = init_1\n",
    "        self.loc_2 = init_2\n",
    "        self.rent_lambda_1 = rent_lambda_1\n",
    "        self.rent_lambda_2 = rent_lambda_2\n",
    "        self.return_lambda_1 = return_lambda_1\n",
    "        self.return_lambda_2 = return_lambda_2\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.poisson_cache = dict()\n",
    "                \n",
    "        # Current Policy moves from Loc 1 to Loc 2.\n",
    "        # For a max/min of +/- 5\n",
    "        \n",
    "        self.actions = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4 ,5]\n",
    "                \n",
    "        # Define state as (i, j) where i = # of cars @ loc 1\n",
    "        # and j = # of cars @ loc 2.\n",
    "        \n",
    "        self.state_values = np.zeros((21, 21))\n",
    "        self.policy = np.zeros((21, 21), dtype=int)\n",
    "    \n",
    "    def get_poisson_value(self, n, lam):\n",
    "        \"\"\"\n",
    "        Return sample drawn from Poisson distribution.\n",
    "        \"\"\"\n",
    "        key = (n, lam)\n",
    "        if key not in self.poisson_cache:\n",
    "            self.poisson_cache[key] = poisson.pmf(n, lam)\n",
    "            \n",
    "        return self.poisson_cache[key]\n",
    "        \n",
    "    \n",
    "    def get_expected_state_reward(self, state, action):\n",
    "        \"\"\"\n",
    "        input the location (1 or 2) and action ([-5, 5])\n",
    "        \n",
    "        Returns state, reward\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "        reward = reward - abs(action)*2\n",
    "        \n",
    "        a = min(state[0] - action, 20)\n",
    "        b = min(state[1] + action, 20)\n",
    "\n",
    "        \"\"\"\n",
    "        Get expected profits for current state, supposing we have restocked from yesterday and\n",
    "        rental requests can be in the range of 1 - 20, with other requests being declined.\n",
    "        \"\"\"\n",
    "        for rent_req_1 in range(11):\n",
    "            for rent_req_2 in range(11):\n",
    "                \n",
    "                prob_rent = self.get_poisson_value(rent_req_1, self.rent_lambda_1) * self.get_poisson_value(rent_req_2, self.rent_lambda_2)\n",
    "                \n",
    "                # Check valid requests\n",
    "                cars_loc_1 = a\n",
    "                cars_loc_2 = b\n",
    "                rentable_loc_1 = min(cars_loc_1, rent_req_1)\n",
    "                rentable_loc_2 = min(cars_loc_2, rent_req_2)\n",
    "\n",
    "                # profit for rental\n",
    "                profit = 10 * (rentable_loc_1 + rentable_loc_2)\n",
    "                cars_loc_1 -= rentable_loc_1\n",
    "                cars_loc_2 -= rentable_loc_2\n",
    "\n",
    "                \"\"\"\n",
    "                Get cars returned for tomorrow:\n",
    "                \"\"\"\n",
    "                \n",
    "                returned_cars_first_loc = self.return_lambda_1\n",
    "                returned_cars_second_loc = self.return_lambda_2\n",
    "                num_of_cars_first_loc = min(cars_loc_1 + returned_cars_first_loc, 20)\n",
    "                num_of_cars_second_loc = min(cars_loc_2 + returned_cars_second_loc, 20)\n",
    "                reward += prob_rent * (profit + self.gamma * self.state_values[num_of_cars_first_loc, num_of_cars_second_loc])\n",
    "                \n",
    "                \"\"\"\n",
    "                for returned_loc_1 in range(11):\n",
    "                    for returned_loc_2 in range(11):\n",
    "\n",
    "                        prob_return = self.get_poisson_value(returned_loc_1, self.return_lambda_1) * self.get_poisson_value(returned_loc_2, self.return_lambda_2)\n",
    "\n",
    "                        new_cars_loc_1 = min(cars_loc_1 + returned_loc_1, 20)\n",
    "                        new_cars_loc_2 = min(cars_loc_2 + returned_loc_2, 20)\n",
    "\n",
    "                        reward += prob_rent*prob_return*(profit + self.gamma*self.state_values[new_cars_loc_1, new_cars_loc_2])\n",
    "                \"\"\"     \n",
    "        return reward\n",
    "                \n",
    "            \n",
    "    def policy_improvement(self, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        @policy:\n",
    "            A numpy 2d array.\n",
    "        @state = (i, j)\n",
    "        \n",
    "        Internally updates self.grid_values to optimal and returns \n",
    "        optimal_state_values (copy of self.grid_values) and optimal_policy\n",
    "        \n",
    "        We take advantage of the fact that the MDP here is deterministic,\n",
    "        meaning given a state and a chosen action, P(s', r | s, a) = 1 or 0. This\n",
    "        is because it mentioned in the question that the agent will go in the direction\n",
    "        as per the action with probability 1. Hence for the action chosen by the policy \n",
    "        we recieve a reward from the `get_new_state_reward()` method with probability 1.\n",
    "        \"\"\"\n",
    "        stable_policy = True\n",
    "        for i in range(21):\n",
    "            for j in range(21):\n",
    "                    \n",
    "                old_action = self.policy[i, j]\n",
    "                new_action = None\n",
    "                prev_max = -np.inf\n",
    "                action_rets = list()\n",
    "                for action in self.actions:\n",
    "                    if (0<=action<=i) or (-j<=action<=0):\n",
    "\n",
    "                        c = self.get_expected_state_reward((i, j), action)\n",
    "                        \n",
    "                        if c > prev_max:\n",
    "                            prev_max = c\n",
    "                            new_action = action\n",
    "\n",
    "                self.policy[i, j] = new_action\n",
    "                \n",
    "                if new_action!=old_action:\n",
    "                    stable_policy = False\n",
    "                    \n",
    "        if stable_policy:\n",
    "            print(\"Optimal Policy:\")\n",
    "            print(self.policy)\n",
    "            return self.policy, True\n",
    "        \n",
    "        else:\n",
    "            print(\"Not stable policy...\")\n",
    "            return self.policy, False\n",
    "                \n",
    "        \n",
    "    def policy_evaluation(self, epsilon=1e-4):\n",
    "        \"\"\"\n",
    "        @policy: \n",
    "            A dict() object containing state keys and optimal action values.\n",
    "            \n",
    "        We again take advantage of the fact that the MDP here is deterministic,\n",
    "        meaning given a state and a chosen action, P(s', r | s, a) = 1 or 0. This\n",
    "        is because it mentioned in the question that the agent will go in the direction\n",
    "        as per the action with probability 1. Hence for the action chosen by the policy \n",
    "        we recieve a reward from the `get_new_state_reward()` method with probability 1.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        Problem: Current evaluation is indefinite because iteratively the policy keeps decreasing \n",
    "        the value and it never converges....\n",
    "        \n",
    "        \"\"\"\n",
    "        flag = [False]*21*21\n",
    "        delta = 0 \n",
    "        counter = 0\n",
    "        while True:\n",
    "            old_values = np.copy(self.state_values)\n",
    "            for i in range(21):\n",
    "                for j in range(21):\n",
    "                        \n",
    "                    c = self.get_expected_state_reward((i, j), self.policy[i, j])\n",
    "                    self.state_values[i, j] = c\n",
    "#                     print(c)\n",
    "\n",
    "            max_value = np.max(abs(old_values - self.state_values))\n",
    "#             print(\"delta:\", max_value)\n",
    "            if max_value<=epsilon:\n",
    "                break\n",
    "            \n",
    "    def policy_iteration(self):\n",
    "        counter = 0\n",
    "        _, axes = plt.subplots(2, 3, figsize=(40, 20))\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        while True:\n",
    "            fig = sns.heatmap(np.flipud(self.policy), cmap=\"YlGnBu\", ax=axes[counter])\n",
    "            fig.set_ylabel('# cars at first location', fontsize=30)\n",
    "            fig.set_yticks(list(reversed(range(20 + 1))))\n",
    "            fig.set_xlabel('# cars at second location', fontsize=30)\n",
    "            fig.set_title('policy {}'.format(counter), fontsize=30)\n",
    "            \n",
    "            print(\"Here we go...\", \"\\n\")\n",
    "            self.policy_evaluation()\n",
    "            a, b = self.policy_improvement()\n",
    "            \n",
    "            if b:\n",
    "                fig = sns.heatmap(np.flipud(self.state_values), cmap=\"YlGnBu\", ax=axes[-1])\n",
    "                fig.set_ylabel('# cars at first location', fontsize=30)\n",
    "                fig.set_yticks(list(reversed(range(20 + 1))))\n",
    "                fig.set_xlabel('# cars at second location', fontsize=30)\n",
    "                fig.set_title('optimal value', fontsize=30)\n",
    "                break\n",
    "                \n",
    "            counter += 1\n",
    "            \n",
    "        plt.show()\n",
    "        plt.savefig('./plots/fig.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we go... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = JacksCarRental()\n",
    "A.policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
